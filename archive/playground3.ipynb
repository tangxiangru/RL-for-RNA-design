{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wow, very many playgrounds\n",
    "# this one is to just strictly work on optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import editdistance\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
    "sys.path.append('/usr/local/ViennaRNA/lib/python3.7/site-packages/')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# from utils.sequence_utils import *\n",
    "from models.Theoretical_models import *\n",
    "from models.Noise_wrapper import *\n",
    "from exploration_strategies.CE import *\n",
    "# from exploration_strategies.RL_explorers_modify import *\n",
    "from utils.landscape_utils import *\n",
    "from models.RNA_landscapes import *\n",
    "from models.Multi_dimensional_model import *\n",
    "import RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACUCAGUACAGAACGGUGGGGGUAUCUUAUACGACCAUAA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "RAA=\"UGCA\" #alphabet\n",
    "length=40\n",
    "wt=generate_random_sequences(length,1,alphabet=RAA)[0]\n",
    "print(wt)\n",
    "#make a simple folding landscape starting at wt\n",
    "landscape1=RNA_landscape(wt)\n",
    "noise_alpha=1\n",
    "virtual_per_measure_ratio=15\n",
    "temperature=0.1\n",
    "# there are multiple abstract \"noise models\" you can use, or you can try to train your own model, using skM\n",
    "noisy_landscape_CE=Noise_wrapper(landscape1,noise_alpha=noise_alpha)\n",
    "noisy_landscape_RL=Noise_wrapper(landscape1,noise_alpha=noise_alpha)\n",
    "noisy_landscape_RL_multiple=Noise_wrapper(landscape1,noise_alpha=noise_alpha)\n",
    "#noisy_landscape=Gaussian_noise_landscape(base_landscape,noise_alpha=0.15)\n",
    "#noisy_landscape=DF_noise_landscape(base_landscape,noise_alpha=0.5)\n",
    "batch_size = 10\n",
    "initial_genotypes=list(set([wt]+[generate_random_mutant(wt,0.05,RAA) for i in range(batch_size*10)]))[:batch_size]\n",
    "len(initial_genotypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGLE AGENT RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import bisect\n",
    "from utils.sequence_utils import translate_string_to_one_hot\n",
    "\n",
    "\n",
    "def renormalize_moves(one_hot_input,rewards_output):\n",
    "    \"\"\"ensures that staying in place gives no reward\"\"\"\n",
    "    zero_current_state=(one_hot_input-1)*-1\n",
    "    return np.multiply(rewards_output,zero_current_state)\n",
    "\n",
    "def walk_away_renormalize_moves(one_hot_input, one_hot_wt, rewards_output):\n",
    "    \"\"\"ensures that moving toward wt is also not useful\"\"\"\n",
    "    zero_current_state=(one_hot_input-1)*-1\n",
    "    zero_wt=((one_hot_wt-1)*-1)\n",
    "    zero_conservative_moves=np.multiply(zero_wt,zero_current_state)\n",
    "    return np.multiply(rewards_output,zero_conservative_moves)\n",
    "\n",
    "def get_all_singles_fitness(model,sequence,alphabet):\n",
    "    prob_singles=np.zeros((len(alphabet),len(sequence)))\n",
    "    for i in range(len(sequence)):\n",
    "        for j in range(len(alphabet)):\n",
    "            putative_seq=sequence[:i]+alphabet[j]+sequence[i+1:]\n",
    "           # print (putative_seq)\n",
    "            prob_singles[j][i]=model.get_fitness(putative_seq)\n",
    "    return prob_singles\n",
    "\n",
    "def sample_greedy(matrix):\n",
    "    i,j=matrix.shape\n",
    "    max_arg=np.argmax(matrix)\n",
    "    y=max_arg%j\n",
    "    x=int(max_arg/j)\n",
    "    output=np.zeros((i,j))\n",
    "    output[x][y]=matrix[x][y]\n",
    "    return output\n",
    "\n",
    "def sample_multi_greedy(matrix):\n",
    "    n = 5 # the number of base positions to greedily change\n",
    "    max_args = np.argpartition(matrix.flatten(), -n)[-n:]\n",
    "    i,j=matrix.shape\n",
    "    output=np.zeros((i,j))\n",
    "    for max_arg in max_args:\n",
    "        y=max_arg%j\n",
    "        x=int(max_arg/j)\n",
    "        output[x][y]=matrix[x][y]\n",
    "    return output\n",
    "\n",
    "def sample_random(matrix):\n",
    "    i,j=matrix.shape\n",
    "    non_zero_moves=np.nonzero(matrix)\n",
    "   # print (non_zero_moves)\n",
    "    k=len(non_zero_moves)\n",
    "    l=len(non_zero_moves[0])\n",
    "    if k!=0 and l!=0:\n",
    "        rand_arg=random.choice([[non_zero_moves[alph][pos] for alph in range(k)] for pos in range(l)])\n",
    "    else:\n",
    "        rand_arg=[random.randint(0,i-1),random.randint(0,j-1)]\n",
    "    #print (rand_arg)\n",
    "    y=rand_arg[1]\n",
    "    x=rand_arg[0]\n",
    "    output=np.zeros((i,j))\n",
    "    output[x][y]=matrix[x][y]\n",
    "    return output   \n",
    "    \n",
    "def construct_mutant_from_sample(pwm_sample, one_hot_base):\n",
    "    one_hot=np.zeros(one_hot_base.shape)\n",
    "    one_hot+=one_hot_base\n",
    "    nonzero = np.nonzero(pwm_sample)\n",
    "    nonzero = list(zip(nonzero[0], nonzero[1]))\n",
    "    for nz in nonzero: # this can be problematic for non-positive fitnesses\n",
    "        i, j = nz\n",
    "        one_hot[:,j]=0\n",
    "        one_hot[i,j]=1\n",
    "    return one_hot\n",
    "\n",
    "def make_one_hot_train_test(genotypes,model,alphabet,split=0.0):\n",
    "    genotypes_one_hot=[translate_string_to_one_hot(genotype,alphabet) for genotype in genotypes]\n",
    "    genotype_fitnesses=[get_all_singles_fitness(model,genotype,alphabet) for genotype in genotypes]\n",
    "    train_x=[]\n",
    "    test_x=[]\n",
    "    train_y=[]\n",
    "    test_y=[]\n",
    "    if split>0: #check this to avoid calling the randomizer if not required\n",
    "        for x,y in zip(genotypes_one_hot,genotype_fitnesses):\n",
    "                if random.random()<split:\n",
    "                    test_x.append(x)\n",
    "                    test_y.append(y)\n",
    "                else:\n",
    "                    train_x.append(x)\n",
    "                    train_y.append(y)\n",
    "        train_x=np.stack(train_x)\n",
    "        train_y=np.stack(train_y)\n",
    "        test_y=np.stack(test_y)\n",
    "        test_x=np.stack(test_x)\n",
    "        return train_x,train_y,test_x,test_y\n",
    "    else:\n",
    "        train_x=np.stack(genotypes_one_hot)\n",
    "        train_y=np.stack(genotype_fitnesses)\n",
    "\n",
    "        return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "# from utils.RL_utils import *\n",
    "from utils.sequence_utils import translate_one_hot_to_string,generate_random_mutant\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class RL_agent_QN():\n",
    "\n",
    "    def __init__(self,start_sequence,alphabet,gamma=0.9,memory_size=1000, device = \"cpu\"):\n",
    "        self.alphabet=alphabet\n",
    "        self.state=translate_string_to_one_hot(start_sequence,self.alphabet)\n",
    "        self.seq_size=len(start_sequence)\n",
    "        self.actor=build_model(self.seq_size,len(self.alphabet), device)\n",
    "        self.start_sequence=translate_string_to_one_hot(start_sequence,self.alphabet)\n",
    "        self.gamma=gamma\n",
    "        self.memory=[]\n",
    "        self.memory_size=memory_size\n",
    "\n",
    "    def reset_position(self,sequence):\n",
    "        self.state=translate_string_to_one_hot(sequence,self.alphabet)\n",
    "\n",
    "    def get_position(self):\n",
    "        return translate_one_hot_to_string(self.state,self.alphabet)\n",
    "\n",
    "    def translate_pwm_to_sequence(self,input_seq_one_hot,output_pwm):\n",
    "        diff=output_pwm-input_seq_one_hot\n",
    "        most_likely=np.argmax(diff,axis=0)\n",
    "        out_seq=\"\"\n",
    "        for m in most_likely:\n",
    "            out_seq+=self.alphabet[m]\n",
    "        return out_seq\n",
    "\n",
    "    def observe(self,landscape,genotypes,costly=False,num_observations=50,epsilon=0.2,depth=10):\n",
    "        if genotypes:\n",
    "            new_sequences=make_one_hot_genotypes(genotypes,self.alphabet)\n",
    "        elif self.memory:\n",
    "            new_sequences=make_one_hot_genotypes(self.memory[-num_observations:],self.alphabet)\n",
    "        else:\n",
    "            return \"Nothing to observe\"\n",
    "        new_moves=[]\n",
    "        all_moves=[]\n",
    "        \n",
    "        #all_fitnesses=[]\n",
    "        for i in range(depth):\n",
    "            for genotype in new_sequences:\n",
    "                if random.random()<epsilon:\n",
    "                    genotype_tensor = torch.from_numpy(np.expand_dims(genotype,axis=0)).float()\n",
    "                    prediction = self.actor(genotype_tensor)\n",
    "                    prediction = prediction.detach().numpy()[0]\n",
    "                    #moves=walk_away_renormalize_moves(genotype,self.start_sequence,prediction)\n",
    "                    moves=renormalize_moves(genotype,prediction)\n",
    "\n",
    "                    sample=sample_random(moves)\n",
    "                    mutant=construct_mutant_from_sample(sample,genotype)\n",
    "                    new_moves.append(translate_one_hot_to_string(mutant,self.alphabet))\n",
    "                else:\n",
    "                    genotype_tensor = torch.from_numpy(np.expand_dims(genotype,axis=0)).float()\n",
    "                    prediction = self.actor(genotype_tensor)\n",
    "                    prediction = prediction.detach().numpy()[0]\n",
    "                    #moves=walk_away_renormalize_moves(genotype,self.start_sequence,prediction)\n",
    "                    moves=renormalize_moves(genotype,prediction)\n",
    "                    \n",
    "                    sample=sample_boltzman(moves)\n",
    "                    mutant=construct_mutant_from_sample(sample,genotype)\n",
    "                    new_moves.append(translate_one_hot_to_string(mutant,self.alphabet))\n",
    "            if costly:\n",
    "                landscape.measure_true_landscape(new_moves)\n",
    "#             else:\n",
    "#                 landscape.genotype_fitnesses(new_moves)\n",
    "\n",
    "            new_sequences=np.stack([translate_string_to_one_hot(move,self.alphabet) for move in new_moves])\n",
    "            all_moves.extend(new_moves)\n",
    "            #all_fitnesses.append(new_fitnesses)\n",
    "            new_moves=[]\n",
    "        self.memory=all_moves[-self.memory_size:] #store at most 1000 memories\n",
    "        return all_moves\n",
    "\n",
    "    def train_actor(self, landscape, observations, train_epochs=10):\n",
    "        loss_fxn = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        total_loss = 0.\n",
    "        for epoch in range(train_epochs):     \n",
    "            moves = observations #random.sample(observations, train_size)\n",
    "            genotypes_one_hot, genotype_fitnesses, t_1, t_2 = make_one_hot_train_test(moves, landscape, self.alphabet)\n",
    "            optimizer.zero_grad()\n",
    "            genotypes_one_hot_tensor = torch.from_numpy(genotypes_one_hot).float()\n",
    "            outputs = self.actor(genotypes_one_hot_tensor)\n",
    "            genotype_fitnesses = torch.tensor(genotype_fitnesses, requires_grad=True).float()\n",
    "            loss = loss_fxn(outputs, genotype_fitnesses)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "#         print('[%d] Avg loss: %.3f' % (epoch+1, total_loss / (epoch + 1)))\n",
    "        return (total_loss / (epoch+1))\n",
    "\n",
    "    def xtrain_actor(self,landscape,observations,train_size=1000,train_epochs=1):\n",
    "        loss_fxn = nn.MSELoss()\n",
    "        optimizer = optim.Adam(net.parameters, lr=0.001)\n",
    "        for epoch in range(train_epochs):  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            moves = random.sample(observations,train_size)+random.sample(self.memory,train_size)\n",
    "            genotypes_one_hot, genotype_fitnesses, t_1, t_2 = make_one_hot_train_test(moves, landscape, self.alphabet)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.actor(genotypes_one_hot)\n",
    "            loss = loss_fxn(outputs, genotype_fitnesses)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 2000))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    def pick_action(self,method):\n",
    "            state_tensor = torch.from_numpy(np.expand_dims(self.state,axis=0)).float()\n",
    "            prediction = self.actor(state_tensor)\n",
    "            prediction = prediction.detach().numpy()[0]\n",
    "            moves=renormalize_moves(self.state,prediction)\n",
    "            \n",
    "            if(method[0] == 'eps'):\n",
    "                epsilon = method[1]\n",
    "                p = random.random()\n",
    "                sample = sample_random(moves) if p < epsilon else sample_greedy(moves)\n",
    "            elif(method[0] == 'multi_eps'):\n",
    "                epsilon = method[1]\n",
    "                p = random.random()\n",
    "                sample = sample_random(moves) if p < epsilon else sample_multi_greedy(moves)\n",
    "            elif(method[0] == 'rollout'):\n",
    "                epsilon = method[1]\n",
    "                p = random.random()\n",
    "                if p < epsilon:\n",
    "                    sample = sample_random(moves)\n",
    "                else:\n",
    "                    n = 5\n",
    "                    for i in range(n):\n",
    "                        sample = sample_greedy(moves)\n",
    "                        mutant = construct_mutant_from_sample(sample,self.state)\n",
    "                        state_tensor = torch.from_numpy(np.expand_dims(mutant,axis=0)).float()\n",
    "                        prediction = self.actor(state_tensor)\n",
    "                        prediction = prediction.detach().numpy()[0]\n",
    "                        moves = renormalize_moves(mutant,prediction)\n",
    "                    sample = sample_greedy(moves)\n",
    "                    \n",
    "            mutant=construct_mutant_from_sample(sample,self.state)\n",
    "            mutant_string=translate_one_hot_to_string(mutant,self.alphabet)\n",
    "\n",
    "            self.state=mutant\n",
    "            self.memory.append(mutant_string)\n",
    "\n",
    "            return mutant_string\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, sequence_len, alphabet_len):\n",
    "        super(Model,self).__init__()\n",
    "        self.sequence_len = sequence_len\n",
    "        self.alphabet_len = alphabet_len\n",
    "        self.linear1 = nn.Linear(alphabet_len * sequence_len, alphabet_len * sequence_len)\n",
    "        self.linear2 = nn.Linear(alphabet_len * sequence_len, sequence_len)\n",
    "        self.linear3 = nn.Linear(sequence_len, sequence_len)\n",
    "        self.linear4 = nn.Linear(sequence_len, 18)\n",
    "        self.linear5 = nn.Linear(18, alphabet_len * sequence_len)\n",
    "        self.linear6 = nn.Linear(alphabet_len * sequence_len, alphabet_len * sequence_len)\n",
    "        self.linear_final = nn.Linear(sequence_len, sequence_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.alphabet_len*self.sequence_len)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.relu(self.linear5(x))\n",
    "        x = F.relu(self.linear6(x))\n",
    "        x = x.view([-1]+list((self.alphabet_len, self.sequence_len)))\n",
    "        x = self.linear_final(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def build_model(sequence_len, alphabet_len, device):\n",
    "    model = Model(sequence_len, alphabet_len).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RL(initial_genotypes, batch_size, generations, sampling_method):\n",
    "    noisy_landscape_RL.reset()\n",
    "    noisy_landscape_RL.measure_true_landscape(initial_genotypes)\n",
    "    noisy_landscape_RL.natural_mode=False\n",
    "    noisy_landscape_RL.local_mode=False\n",
    "    noisy_landscape_RL.cost\n",
    "\n",
    "    agent = RL_agent_QN(wt,alphabet=RAA,memory_size=10000, device=device)\n",
    "    observations=[x for x in initial_genotypes]#agent.observe(noisy_landscape_RL,initial_genotypes,costly=True,num_observations=100,depth=5)\n",
    "    agent.train_actor(noisy_landscape_RL,observations, train_epochs=10)\n",
    "    new_sequences = sorted([(noisy_landscape_RL.get_fitness(observations[i]),[observations[i]])\n",
    "                            for i in range(len(observations))])\n",
    "\n",
    "    RL_top_seqs = []\n",
    "    while noisy_landscape_RL.cost<batch_size*generations:\n",
    "        if (sampling_method == 'eps'):\n",
    "            eps = max(0.2,(0.5 - noisy_landscape_RL.cost / (batch_size * generations)))\n",
    "            method = (sampling_method, eps)\n",
    "        elif (sampling_method == 'multi_eps'):\n",
    "            eps = max(0.2,(0.5 - noisy_landscape_RL.cost / (batch_size * generations)))\n",
    "            method = (sampling_method, eps)\n",
    "        elif (sampling_method == 'rollout'):\n",
    "            eps = max(0.2,(0.5 - noisy_landscape_RL.cost / (batch_size * generations)))\n",
    "            method = (sampling_method, eps)\n",
    "        else:\n",
    "            method = (sampling_method,)\n",
    "        \n",
    "        b = 0\n",
    "        new = []\n",
    "        while(b < batch_size):\n",
    "            s = agent.pick_action(method)\n",
    "            if not s in noisy_landscape_RL.measured_sequences.keys():\n",
    "                new += [s]\n",
    "                b += 1\n",
    "    #     new=[agent.pick_action_epsilon_greedy(epsilon=eps) for _ in range(batch_size)]\n",
    "        noisy_landscape_RL.measure_true_landscape(new)\n",
    "        observations += new\n",
    "        avg_loss = agent.train_actor(noisy_landscape_RL,observations, train_epochs=20)\n",
    "        print (noisy_landscape_RL.cost, noisy_landscape_RL.get_fitness(new_sequences[-1][1][0]), avg_loss)\n",
    "        new_sequences += [(noisy_landscape_RL.get_fitness(new[i]),[new[i]]) for i in range(len(new))]\n",
    "        new_sequences = sorted(new_sequences)\n",
    "\n",
    "        RL_top_seqs += [(noisy_landscape_RL.cost, new_sequences[-1][0])]\n",
    "    return RL_top_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 0.10941176694982192 0.005418052885215729\n",
      "28 0.10941176694982192 0.0019043981097638606\n",
      "38 0.18117646610035615 0.0007166770636104047\n",
      "47 0.18117646610035615 0.0007371568994130939\n",
      "53 0.18117646610035615 0.0005877455972949974\n",
      "59 0.19411764705882353 0.0010119560349266977\n",
      "62 0.3105882308062385 0.0007589524320792407\n",
      "66 0.39176469690659466 0.001146218180656433\n",
      "72 0.44352942074046414 0.0007976353954290971\n",
      "80 0.44352942074046414 0.0007265045889653266\n",
      "89 0.44352942074046414 0.0008908253425033763\n",
      "99 0.44352942074046414 0.0007590197856188752\n",
      "102 0.4658823349896599 0.0008459628035780043\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "generations = 10\n",
    "RL_data = run_RL(initial_genotypes, batch_size, generations, 'multi_eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI AGENT RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RL_multi_agent_QN():\n",
    "    def __init__(self,agent_type,landscape,wt,alphabet, pop_size,memory_size=1200,best_seq_prop=1):\n",
    "        self.memory=[]\n",
    "        self.pop_size=pop_size\n",
    "        self.alphabet=alphabet\n",
    "        self.wt=wt\n",
    "        self.agents=[agent_type(self.wt,self.alphabet) for i in range(self.pop_size)]\n",
    "        self.memory_size=memory_size\n",
    "        self.memory=[]\n",
    "        self.landscape=landscape\n",
    "        self.best_seqs=[]\n",
    "        self.best_seqs_len=self.pop_size*best_seq_prop\n",
    "\n",
    "    def add_to_top_seqs(self,sequence):\n",
    "        fitness= self.landscape.get_fitness(sequence)\n",
    "        if len(self.best_seqs)<self.best_seqs_len:\n",
    "            self.best_seqs.append((fitness,sequence))\n",
    "\n",
    "        else:\n",
    "            if fitness > self.best_seqs[0][0] and (fitness,sequence) not in self.best_seqs:\n",
    "                self.best_seqs.append((fitness,sequence))\n",
    "                self.best_seqs=sorted(self.best_seqs)\n",
    "                self.best_seqs=self.best_seqs[1:]\n",
    "\n",
    "    def reset_position(self,sequence):\n",
    "        for agent in self.agents:\n",
    "            agent.reset_position(sequence)\n",
    "\n",
    "    def observe(self, num_observations=1000, costly=False, depth=10):\n",
    "        for agent in self.agents:\n",
    "            observations=agent.observe(self.landscape,[generate_random_mutant(agent.get_position(),0.1,self.alphabet) for i in range(num_observations)],costly=True,depth=depth)\n",
    "            self.memory.extend(observations)\n",
    "        if len(self.memory)>self.memory_size:\n",
    "            self.memory=random.sample(self.memory,self.memory_size)\n",
    "            \n",
    "    def force_observations(self, observations):\n",
    "        num_agents = len(self.agents)\n",
    "        for agent in self.agents:\n",
    "            agent.memory.extend(observations)\n",
    "            if len(agent.memory) > agent.memory_size:\n",
    "                agent.memory = random.sample(agent.memory, agent.memory_size)\n",
    "\n",
    "    def train_agents(self, observations):\n",
    "        total_loss = 0.0\n",
    "        for agent in self.agents:\n",
    "            total_loss += agent.train_actor(self.landscape, observations, train_epochs=20)\n",
    "        return total_loss/len(self.agents)\n",
    "\n",
    "    def cross_train_agents(self, observations, num_x_obs=1000):\n",
    "        total_loss = 0.0\n",
    "        for agent in self.agents:\n",
    "            total_loss += agent.xtrain_actor(self.landscape, random.sample(observations,num_x_obs), train_size=max(len(observations), num_x_obs), train_epochs=20)\n",
    "        return total_loss/len(self.agents)\n",
    "\n",
    "    def pick_action(self, method):\n",
    "        self.sequences=[]\n",
    "        sequences=[]\n",
    "        for agent_id, agent in enumerate(self.agents):\n",
    "            action = agent.pick_action(method)\n",
    "            self.sequences.append(action)\n",
    "            sequences.append((action, agent_id))\n",
    "#             self.memory.extend(agent.memory)\n",
    "\n",
    "#         if len(self.memory)>self.memory_size:\n",
    "#             self.memory=random.sample(self.memory,self.memory_size)\n",
    "\n",
    "        for seq in self.sequences:\n",
    "            self.add_to_top_seqs(seq)\n",
    "\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RL_multi_agent(initial_genotypes, batch_size, generations, sampling_method, num_agents=10):\n",
    "    noisy_landscape_RL_multiple.reset()\n",
    "    noisy_landscape_RL_multiple.measure_true_landscape(initial_genotypes)\n",
    "    noisy_landscape_RL_multiple.natural_mode=False\n",
    "    noisy_landscape_RL_multiple.local_mode=False\n",
    "    noisy_landscape_RL_multiple.cost\n",
    "\n",
    "    multi_agents = RL_multi_agent_QN(RL_agent_QN,\n",
    "                              noisy_landscape_RL_multiple,\n",
    "                              wt,\n",
    "                              RAA,\n",
    "                              num_agents,\n",
    "                              memory_size=batch_size)\n",
    "\n",
    "    # set the SAME initial position for each\n",
    "    for i in range(num_agents):\n",
    "        multi_agents.agents[i].reset_position(initial_genotypes[0])\n",
    "\n",
    "    # keep separate list of observations for each agent\n",
    "    observations = [[x for x in initial_genotypes] for i in range(num_agents)]\n",
    "    new_sequences = sorted([(noisy_landscape_RL_multiple.get_fitness(observations[i]),[observations[i]])\n",
    "                            for i in range(len(observations))])\n",
    "\n",
    "    multi_RL_top_seqs = []\n",
    "    \n",
    "    while noisy_landscape_RL_multiple.cost < batch_size * generations:\n",
    "        if (sampling_method == 'eps'):\n",
    "            eps = max(0.2,(0.5 - noisy_landscape_RL.cost / (batch_size * generations)))\n",
    "            method = (sampling_method, eps)\n",
    "        elif (sampling_method == 'multi_eps'):\n",
    "            eps = max(0.2,(0.5 - noisy_landscape_RL.cost / (batch_size * generations)))\n",
    "            method = (sampling_method, eps)\n",
    "        elif (sampling_method == 'rollout'):\n",
    "            eps = max(0.2,(0.5 - noisy_landscape_RL.cost / (batch_size * generations)))\n",
    "            method = (sampling_method, eps)\n",
    "        else:\n",
    "            method = (sampling_method,)\n",
    "        \n",
    "        # DOESN'T WORK: goal; train each agent on their own observations, and a subsample of other agents observations\n",
    "        b = 0\n",
    "        new = []\n",
    "        new_indices = []\n",
    "        while(b < batch_size):\n",
    "            seqs = multi_agents.pick_action(method)\n",
    "            for s in seqs:\n",
    "                if not s[0] in noisy_landscape_RL_multiple.measured_sequences.keys():\n",
    "                    new += [s[0]]\n",
    "                    new_indices += [s[1]]\n",
    "                    b += 1\n",
    "\n",
    "        noisy_landscape_RL_multiple.measure_true_landscape(new)\n",
    "        observations += new\n",
    "    #     multi_agents.force_observations(observations)\n",
    "        avg_loss = multi_agents.train_agents(observations)\n",
    "        print(noisy_landscape_RL_multiple.cost, noisy_landscape_RL_multiple.get_fitness(new_sequences[-1][1][0]), avg_loss)\n",
    "        new_sequences += [(noisy_landscape_RL_multiple.get_fitness(new[i]),[new[i]]) for i in range(len(new))]\n",
    "        new_sequences = sorted(new_sequences)\n",
    "        multi_RL_top_seqs += [(noisy_landscape_RL_multiple.cost, new_sequences[-1][0])]\n",
    "        \n",
    "    return multi_RL_top_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-83ca16c53934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_agents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mRL_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_RL_multi_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_genotypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'multi_eps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c6ce6fe80c49>\u001b[0m in \u001b[0;36mrun_RL_multi_agent\u001b[0;34m(initial_genotypes, batch_size, generations, sampling_method, num_agents)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minitial_genotypes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     new_sequences = sorted([(noisy_landscape_RL_multiple.get_fitness(observations[i]),[observations[i]])\n\u001b[0;32m---> 22\u001b[0;31m                             for i in range(len(observations))])\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmulti_RL_top_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c6ce6fe80c49>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minitial_genotypes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     new_sequences = sorted([(noisy_landscape_RL_multiple.get_fitness(observations[i]),[observations[i]])\n\u001b[0;32m---> 22\u001b[0;31m                             for i in range(len(observations))])\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmulti_RL_top_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FLRL/models/Noise_wrapper.py\u001b[0m in \u001b[0;36mget_fitness\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasured_sequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasured_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_sequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "generations = 10\n",
    "num_agents = 2\n",
    "RL_data = run_RL_multi_agent(initial_genotypes, batch_size, generations, 'multi_eps', num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
