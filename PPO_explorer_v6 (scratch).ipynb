{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import bisect\n",
    "import copy \n",
    "import os \n",
    "from collections import deque, Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import editdistance\n",
    "import sys\n",
    "import RNA\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# import path \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.sequence_utils import translate_one_hot_to_string,generate_random_mutant\n",
    "from utils.sequence_utils import translate_string_to_one_hot, translate_one_hot_to_string\n",
    "from models.Theoretical_models import *\n",
    "from models.Noise_wrapper import *\n",
    "from utils.landscape_utils import *\n",
    "from models.RNA_landscapes import *\n",
    "from models.Multi_dimensional_model import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.agents.ppo import ppo_policy, ppo_agent, ppo_utils\n",
    "from tf_agents.environments import py_environment, tf_py_environment\n",
    "from tf_agents.environments.utils import validate_py_environment\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.networks import network, normal_projection_network\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAA=\"UGCA\" #alphabet\n",
    "alphabet_len=len(RAA)\n",
    "length=20\n",
    "noise_alpha=1\n",
    "generations = 10\n",
    "experiment_batch_size = 100\n",
    "\n",
    "wt=generate_random_sequences(length,1,alphabet=RAA)[0]\n",
    "landscape=RNA_landscape(wt)\n",
    "noisy_landscape=Noise_wrapper(landscape,\n",
    "                              noise_alpha=noise_alpha,\n",
    "                              always_costly=True)\n",
    "initial_genotypes=list(set([wt]+[generate_random_mutant(wt,0.05,RAA) \n",
    "                                 for i in range(experiment_batch_size*10)]))[:experiment_batch_size]\n",
    "noisy_landscape.reset()\n",
    "noisy_landscape.measure_true_landscape(initial_genotypes)\n",
    "noisy_landscape.natural_mode=False\n",
    "noisy_landscape.local_mode=False\n",
    "noisy_landscape.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitnessLandscapeEnvironment(py_environment.PyEnvironment):\n",
    "    def __init__(self, alphabet, seq_len,\n",
    "                 landscape, max_episodes):\n",
    "        self.alphabet = alphabet\n",
    "        self.alphabet_len = len(alphabet)\n",
    "        # should we deepcopy here?\n",
    "        self.landscape = copy.deepcopy(landscape)\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # we really shouldn't have ints, since it doesn't make\n",
    "        # sense to say \"28\" and \"29\" should be closer than\n",
    "        # \"28\" and \"40\", but I'm just trying to get anything\n",
    "        # to work...\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), \n",
    "            dtype=np.int32, \n",
    "            minimum=0, maximum=self.alphabet_len*self.seq_len-1, \n",
    "            name='action_x'\n",
    "        )\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(self.alphabet_len, self.seq_len),\n",
    "            dtype=np.int32,\n",
    "            minimum=0, maximum=1,\n",
    "            name='observation'\n",
    "        )\n",
    "        self._time_step_spec = ts.time_step_spec(\n",
    "            self._observation_spec)\n",
    "        # initialize state\n",
    "        self._state = translate_string_to_one_hot(wt,\n",
    "                                                  self.alphabet)\n",
    "\n",
    "        # RL housekeeping\n",
    "        self._episode_ended = False\n",
    "        self.counter = 0\n",
    "        self.max_episodes = max_episodes\n",
    "        self.seen_sequences = {}\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.counter = 0\n",
    "        # there's no reason why we should expect the model to do different, random moves for the same\n",
    "        # starting point... so why not start at a different one?\n",
    "#         self._state = translate_string_to_one_hot(wt, self.alphabet)\n",
    "        self._state = translate_string_to_one_hot(generate_random_sequences(length,1,alphabet=RAA)[0], self.alphabet)\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "    \n",
    "    # spec housekeeping\n",
    "    def time_step_spec(self):\n",
    "        return self._time_step_spec\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def _step(self, action):\n",
    "        if self.counter >= self.max_episodes:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array(self._state,\n",
    "                                           dtype=np.int32),\n",
    "                                  reward=0)\n",
    "        self.counter += 1\n",
    "        \n",
    "        print(\"action\", action)\n",
    "        # derive what base is modified in which position\n",
    "        action_one_hot = np.zeros((self.alphabet_len,\n",
    "                                  self.seq_len))\n",
    "        base, pos = action//self.seq_len, action%self.seq_len\n",
    "        action_one_hot[base, pos] = 1\n",
    "        \n",
    "        # if we're trying to make a no-op this is bad\n",
    "        if self._state[base, pos] == 1:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array(self._state,\n",
    "                                           dtype=np.int32),\n",
    "                                  reward=-1)\n",
    "        \n",
    "        self._state = construct_mutant_from_sample(action_one_hot,\n",
    "                                                  self._state)\n",
    "        state_string = translate_one_hot_to_string(self._state,\n",
    "                                                   self.alphabet)\n",
    "        \n",
    "        # if we've seen the new state string before, end the episode\n",
    "        if state_string in self.seen_sequences:\n",
    "            return ts.termination(np.array(self._state,\n",
    "                                           dtype=np.int32),\n",
    "                                  reward=-1)\n",
    "        self.seen_sequences[state_string] = 1\n",
    "        \n",
    "        reward = self.landscape.get_fitness(state_string)\n",
    "        return ts.transition(np.array(self._state,\n",
    "                                      dtype=np.int32),\n",
    "                             reward=reward*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 10**6\n",
    "fle = FitnessLandscapeEnvironment(RAA, length, landscape, max_episodes)\n",
    "validate_py_environment(fle, episodes=1)\n",
    "tf_env = tf_py_environment.TFPyEnvironment(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specs \n",
    "time_step_spec = tf_env.time_step_spec()\n",
    "observation_spec = tf_env.observation_spec()\n",
    "action_spec = tf_env.action_spec()\n",
    "alphabet_len = len(RAA)\n",
    "seq_len = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     12
    ]
   },
   "outputs": [],
   "source": [
    "# # run random agent for testing purposes only \n",
    "# random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec())\n",
    "# random_collect_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec())\n",
    "# random_agent = tf_agent.TFAgent(\n",
    "#     tf_env.time_step_spec(),\n",
    "#     tf_env.action_spec(),\n",
    "#     random_policy,\n",
    "#     random_collect_policy,\n",
    "#     None\n",
    "# )\n",
    "\n",
    "# collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "#     tf_env,\n",
    "#     random_agent.collect_policy,\n",
    "#     num_steps=100\n",
    "# )\n",
    "\n",
    "# collect_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import actor_distribution_network, value_network\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=(128,)\n",
    ")\n",
    "value_net = value_network.ValueNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    fc_layer_params=(40, 5)\n",
    ")\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-5)\n",
    "\n",
    "agent = ppo_agent.PPOAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    optimizer,\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100)\n",
    "\n",
    "def collect_training_data():\n",
    "    dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env,\n",
    "        agent.collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=10).run()\n",
    "    \n",
    "def train_agent():\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        sample_batch_size=100,\n",
    "        num_steps=2)\n",
    "\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    loss = None\n",
    "    for _ in range(10):\n",
    "        trajectories, _ = next(iterator)\n",
    "        loss = agent.train(experience=trajectories)\n",
    "\n",
    "    print('Training loss: ', loss.loss.numpy())\n",
    "    return loss.loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_py_metric\n",
    "from tf_agents.metrics import py_metric\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "\n",
    "class MaxEpisodeScoreMetric(py_metric.PyStepMetric):\n",
    "    def __init__(self, name='MaxEpisodeScoreMetric'):\n",
    "        super(py_metric.PyStepMetric, self).__init__(name)\n",
    "        self.rewards = []\n",
    "        self.discounts = []\n",
    "        self.max_discounted_reward = None\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        self.discounts = []\n",
    "        self.max_discounted_reward = None\n",
    "        \n",
    "    def call(self, trajectory):\n",
    "        self.rewards += trajectory.reward\n",
    "        self.discounts += trajectory.discount\n",
    "    \n",
    "        if (trajectory.is_last()):\n",
    "            adjusted_discounts = [1.0] + self.discounts # because a step has its value + the discount of the NEXT step (Bellman equation)\n",
    "            adjusted_discounts = adjusted_discounts[:-1] # dropping the discount of the last step because it is not followed by a next step, so the value is useless\n",
    "            discounted_reward = np.sum(np.multiply(self.rewards, adjusted_discounts))\n",
    "            print(self.rewards, adjusted_discounts, discounted_reward)\n",
    "\n",
    "            if self.max_discounted_reward == None:\n",
    "                self.max_discounted_reward = discounted_reward\n",
    "\n",
    "            if discounted_reward > self.max_discounted_reward:\n",
    "                self.max_discounted_reward = discounted_reward\n",
    "                \n",
    "            self.rewards = []\n",
    "            self.discounts = []\n",
    "    \n",
    "    def result(self):\n",
    "        return self.max_discounted_reward\n",
    "    \n",
    "class TFMaxEpisodeScoreMetric(tf_py_metric.TFPyMetric):\n",
    "    def __init__(self, name='MaxEpisodeScoreMetric', dtype=tf.float32):\n",
    "        py_metric = MaxEpisodeScoreMetric()\n",
    "\n",
    "        super(TFMaxEpisodeScoreMetric, self).__init__(\n",
    "            py_metric=py_metric, name=name, dtype=dtype)\n",
    "\n",
    "def evaluate_agent():\n",
    "    max_score = TFMaxEpisodeScoreMetric() # a class from the article mentioned at the beginning\n",
    "    observers = [max_score]\n",
    "    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env, agent.policy, observers, num_episodes=100)\n",
    "\n",
    "    final_time_step, policy_state = driver.run()\n",
    "\n",
    "    print('Max test score:', max_score.result().numpy())\n",
    "    return max_score.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_training_data()\n",
    "# train_agent()\n",
    "dataset = replay_buffer.as_dataset(\n",
    "        sample_batch_size=1000,\n",
    "        num_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('Step ', i)\n",
    "    collect_training_data()\n",
    "    train_agent()\n",
    "    evaluate_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how agent performs \n",
    "time_step = tf_env.reset()\n",
    "for _ in range(50):\n",
    "    tf_env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    while not tf_env.current_time_step().is_last():\n",
    "        action = agent.collect_policy.action(tf_env.current_time_step()).action\n",
    "        print('predicted action', action)\n",
    "        next_time_step = tf_env.step(action)\n",
    "        episode_steps += 1\n",
    "        print(\"Reward\", next_time_step.reward.numpy())\n",
    "        episode_reward += next_time_step.reward.numpy()\n",
    "    print(\"Steps:\", episode_steps, \"Reward:\", episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                            time_step_spec=tf_env.time_step_spec())\n",
    "\n",
    "max_score = TFMaxEpisodeScoreMetric()\n",
    "\n",
    "observers = [max_score]\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(tf_env, tf_policy, observers, num_episodes=1000)\n",
    "\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('Max score:', max_score.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
