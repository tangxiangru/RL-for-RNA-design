{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os \n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import editdistance\n",
    "import sys\n",
    "import RNA\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# import path \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.sequence_utils import translate_one_hot_to_string,generate_random_mutant\n",
    "from models.Theoretical_models import *\n",
    "from models.Noise_wrapper import *\n",
    "from exploration_strategies.CE import *\n",
    "from utils.landscape_utils import *\n",
    "from models.RNA_landscapes import *\n",
    "from models.Multi_dimensional_model import *\n",
    "\n",
    "from segment_tree import MinSegmentTree, SumSegmentTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAA=\"UGCA\" #alphabet\n",
    "length=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUCAACGAGCGCCAAGGUUCUGCCUAUUCGACCGUUGUUC'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt=generate_random_sequences(length,1,alphabet=RAA)[0]\n",
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a simple folding landscape starting at wt\n",
    "landscape1=RNA_landscape(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_alpha=1\n",
    "experiment_batch_size=10\n",
    "virtual_per_measure_ratio=15\n",
    "temperature=0.1\n",
    "generations=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are multiple abstract \"noise models\" you can use, or you can try to train your own model, using skM\n",
    "noisy_landscape=Noise_wrapper(landscape1,\n",
    "                              noise_alpha=noise_alpha,\n",
    "                              always_costly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_genotypes=list(set([wt]+[generate_random_mutant(wt,0.05,RAA) for i in range(experiment_batch_size*10)]))[:experiment_batch_size]\n",
    "len(initial_genotypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_landscape.reset()\n",
    "noisy_landscape.measure_true_landscape(initial_genotypes)\n",
    "noisy_landscape.natural_mode=False\n",
    "noisy_landscape.local_mode=False\n",
    "noisy_landscape.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import bisect\n",
    "from utils.sequence_utils import translate_string_to_one_hot, translate_one_hot_to_string\n",
    "\n",
    "def renormalize_moves(one_hot_input, rewards_output):\n",
    "    \"\"\"ensures that staying in place gives no reward\"\"\"\n",
    "    zero_current_state = (one_hot_input - 1) * (-1)\n",
    "    return np.multiply(rewards_output, zero_current_state)\n",
    "\n",
    "def walk_away_renormalize_moves(one_hot_input, one_hot_wt, rewards_output):\n",
    "    \"\"\"ensures that moving toward wt is also not useful\"\"\"\n",
    "    zero_current_state=(one_hot_input-1)*-1\n",
    "    zero_wt=((one_hot_wt-1)*-1)\n",
    "    zero_conservative_moves=np.multiply(zero_wt,zero_current_state)\n",
    "    return np.multiply(rewards_output,zero_conservative_moves)\n",
    "\n",
    "def get_all_singles_fitness(model,sequence,alphabet):\n",
    "    prob_singles=np.zeros((len(alphabet),len(sequence)))\n",
    "    for i in range(len(sequence)):\n",
    "        for j in range(len(alphabet)):\n",
    "            putative_seq=sequence[:i]+alphabet[j]+sequence[i+1:]\n",
    "           # print (putative_seq)\n",
    "            prob_singles[j][i]=model.get_fitness(putative_seq)\n",
    "    return prob_singles\n",
    "\n",
    "def get_all_mutants(sequence):\n",
    "    mutants = []\n",
    "    for i in range(sequence.shape[0]):\n",
    "        for j in range(sequence.shape[1]):\n",
    "            putative_seq = sequence.copy()\n",
    "            putative_seq[:, j] = 0\n",
    "            putative_seq[i, j] = 1\n",
    "            mutants.append(putative_seq)\n",
    "    return np.array(mutants)\n",
    "\n",
    "def sample_greedy(matrix):\n",
    "    i,j=matrix.shape\n",
    "    max_arg=np.argmax(matrix)\n",
    "    y=max_arg%j\n",
    "    x=int(max_arg/j)\n",
    "    output=np.zeros((i,j))\n",
    "    output[x][y]=matrix[x][y]\n",
    "    return output\n",
    "\n",
    "def sample_multi_greedy(matrix):\n",
    "    n = 5 # the number of base positions to greedily change\n",
    "    max_args = np.argpartition(matrix.flatten(), -n)[-n:]\n",
    "    i,j=matrix.shape\n",
    "    output=np.zeros((i,j))\n",
    "    for max_arg in max_args:\n",
    "        y=max_arg%j\n",
    "        x=int(max_arg/j)\n",
    "        output[x][y]=matrix[x][y]\n",
    "    return output\n",
    "\n",
    "def sample_random(matrix):\n",
    "    i,j=matrix.shape\n",
    "    non_zero_moves=np.nonzero(matrix)\n",
    "   # print (non_zero_moves)\n",
    "    k=len(non_zero_moves)\n",
    "    l=len(non_zero_moves[0])\n",
    "    if k!=0 and l!=0:\n",
    "        rand_arg=random.choice([[non_zero_moves[alph][pos] for alph in range(k)] for pos in range(l)])\n",
    "    else:\n",
    "        rand_arg=[random.randint(0,i-1),random.randint(0,j-1)]\n",
    "    #print (rand_arg)\n",
    "    y=rand_arg[1]\n",
    "    x=rand_arg[0]\n",
    "    output=np.zeros((i,j))\n",
    "    output[x][y] = 1\n",
    "    return output   \n",
    "\n",
    "def action_to_scalar(matrix):\n",
    "    matrix = matrix.ravel()\n",
    "    for i in range(len(matrix)):\n",
    "        if matrix[i] != 0:\n",
    "            return i\n",
    "    \n",
    "def construct_mutant_from_sample(pwm_sample, one_hot_base):\n",
    "    one_hot = np.zeros(one_hot_base.shape)\n",
    "    one_hot += one_hot_base\n",
    "    nonzero = np.nonzero(pwm_sample)\n",
    "    nonzero = list(zip(nonzero[0], nonzero[1]))\n",
    "    for nz in nonzero: # this can be problematic for non-positive fitnesses\n",
    "        i, j = nz\n",
    "        one_hot[:,j]=0\n",
    "        one_hot[i,j]=1\n",
    "    return one_hot\n",
    "\n",
    "def best_predicted_new_gen(actor, genotypes, alphabet, pop_size):\n",
    "    mutants = get_all_mutants(genotypes)\n",
    "    one_hot_mutants = np.array([translate_string_to_one_hot(mutant, alphabet) for mutant in mutants])\n",
    "    torch_one_hot_mutants = torch.from_numpy(np.expand_dims(one_hot_mutants, axis=0)).float()\n",
    "    predictions = actor(torch_one_hot_mutants)\n",
    "    predictions = predictions.detach().numpy()\n",
    "    best_pred_ind = predictions.argsort()[-pop_size:]\n",
    "    return mutants[best_pred_ind]\n",
    "\n",
    "def make_one_hot_train_test(genotypes, model, alphabet):\n",
    "    genotypes_one_hot = np.array([translate_string_to_one_hot(genotype, alphabet) for genotype in genotypes])\n",
    "    genotype_fitnesses = []\n",
    "    for genotype in genotypes:\n",
    "        genotype_fitnesses.append(model.get_fitness(genotype))\n",
    "    genotype_fitnesses = np.array(genotype_fitnesses)\n",
    "\n",
    "    return genotypes_one_hot, genotype_fitnesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 128):\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray\n",
    "    ):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        return dict(obs=self.obs_buf[idxs],\n",
    "                    next_obs=self.next_obs_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    \"\"\"Prioritized Replay buffer.\n",
    "    \n",
    "    Attributes:\n",
    "        max_priority (float): max priority\n",
    "        tree_ptr (int): next index of tree\n",
    "        alpha (float): alpha parameter for prioritized replay buffer\n",
    "        sum_tree (SumSegmentTree): sum tree for prior\n",
    "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim: int,\n",
    "        size: int, \n",
    "        batch_size: int = 32, \n",
    "        alpha: float = 0.6\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        assert alpha >= 0\n",
    "        \n",
    "        super(PrioritizedReplayBuffer, self).__init__(obs_dim, size, batch_size)\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "        \n",
    "    def store(\n",
    "        self, \n",
    "        obs: np.ndarray, \n",
    "        act: int, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray\n",
    "    ):\n",
    "        \"\"\"Store experience and priority.\"\"\"\n",
    "        super().store(obs, act, rew, next_obs)\n",
    "        \n",
    "        self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "        self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "        self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "\n",
    "    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "        \n",
    "        indices = self._sample_proportional()\n",
    "        \n",
    "        obs = self.obs_buf[indices]\n",
    "        next_obs = self.next_obs_buf[indices]\n",
    "        acts = self.acts_buf[indices]\n",
    "        rews = self.rews_buf[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        \n",
    "        return dict(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            acts=acts,\n",
    "            rews=rews,\n",
    "\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "        \n",
    "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
    "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "            \n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        \"\"\"Sample indices based on proportions.\"\"\"\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment = p_total / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def _calculate_weight(self, idx: int, beta: float):\n",
    "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "        \n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "# from utils.RL_utils import *\n",
    "from utils.sequence_utils import translate_one_hot_to_string,generate_random_mutant\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "    def __init__(self, sequence_len, alphabet_len):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.sequence_len = sequence_len\n",
    "        self.alphabet_len = alphabet_len\n",
    "        self.linear1 = nn.Linear(2 * alphabet_len * sequence_len, alphabet_len * sequence_len)\n",
    "        self.bn1 = nn.BatchNorm1d(alphabet_len * sequence_len)\n",
    "        self.linear2 = nn.Linear(alphabet_len * sequence_len, sequence_len)\n",
    "        self.bn2 = nn.BatchNorm1d(sequence_len)\n",
    "        self.linear3 = nn.Linear(sequence_len, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.bn2(F.relu(self.linear2(x)))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        return x\n",
    "    \n",
    "def build_q_network(sequence_len, alphabet_len, device):\n",
    "    model = Q_Network(sequence_len, alphabet_len).to(device)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "class RL_agent_DQN():\n",
    "    '''\n",
    "    Based off https://colab.research.google.com/drive/1NsbSPn6jOcaJB_mp9TmkgQX7UrRIrTi0\n",
    "    '''\n",
    "    def __init__(self, landscape, start_sequence, alphabet, gamma=0.9, \n",
    "                 memory_size=100000, batch_size=10, experiment_batch_size=10,\n",
    "                 device = \"cpu\", noise_alpha=1):\n",
    "        self.alphabet = alphabet\n",
    "        self.alphabet_size = len(alphabet)\n",
    "        self.state = translate_string_to_one_hot(start_sequence, self.alphabet)\n",
    "        self.seq_size = len(start_sequence)\n",
    "        self.q_network = build_q_network(self.seq_size, len(self.alphabet), device)\n",
    "        self.q_network.eval()\n",
    "        self.start_sequence = translate_string_to_one_hot(start_sequence,self.alphabet)\n",
    "        self.memory_size = memory_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.experiment_batch_size = experiment_batch_size\n",
    "        self.memory = PrioritizedReplayBuffer(len(self.alphabet) * self.seq_size, \n",
    "                                              memory_size, batch_size, 0.6)\n",
    "        self.seen_sequences = []\n",
    "        self.landscape = landscape\n",
    "        self.best_fitness = 0\n",
    "        \n",
    "        self.train_epochs = 10\n",
    "        self.epsilon_min = 0.1\n",
    "        \n",
    "        self.top_sequence = []\n",
    "\n",
    "    def reset_position(self,sequence):\n",
    "        self.state=translate_string_to_one_hot(sequence,self.alphabet)\n",
    "\n",
    "    def get_position(self):\n",
    "        return translate_one_hot_to_string(self.state,self.alphabet)\n",
    "\n",
    "    def translate_pwm_to_sequence(self,input_seq_one_hot,output_pwm):\n",
    "        diff=output_pwm-input_seq_one_hot\n",
    "        most_likely=np.argmax(diff,axis=0)\n",
    "        out_seq=\"\"\n",
    "        for m in most_likely:\n",
    "            out_seq+=self.alphabet[m]\n",
    "        return out_seq\n",
    "    \n",
    "    def sample(self):\n",
    "        indices = np.random.choice(len(self.memory), self.batch_size)\n",
    "        rewards, actions, states, next_states = zip(*[self.memory[ind] for ind in indices])\n",
    "        return np.array(rewards), np.array(actions), np.array(states), np.array(next_states) \n",
    "    \n",
    "    def calculate_next_q_values(self, state_v):\n",
    "        dim = self.alphabet_size * self.seq_size\n",
    "        states_repeated = state_v.repeat(1, dim).reshape(-1, dim)\n",
    "        actions_repeated = torch.FloatTensor(np.identity(dim)).repeat(len(state_v), 1)\n",
    "        next_states_actions = torch.cat((states_repeated, actions_repeated), 1)\n",
    "        next_states_values = self.q_network(next_states_actions)\n",
    "        next_states_values = next_states_values.reshape(len(state_v), -1)\n",
    "        \n",
    "        return next_states_values\n",
    "    \n",
    "    def q_network_loss(self, batch, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Calculate MSE between actual state action values,\n",
    "        and expected state action values from DQN\n",
    "        \"\"\"\n",
    "        rewards, actions, states, next_states = \\\n",
    "        batch['rews'], batch['acts'], batch['obs'], batch['next_obs']\n",
    "        \n",
    "        state_action_v = torch.FloatTensor(np.hstack((states, actions)))\n",
    "        rewards_v = torch.FloatTensor(rewards)\n",
    "        next_states_v = torch.FloatTensor(next_states)\n",
    "    \n",
    "        state_action_values = self.q_network(state_action_v).view(-1)\n",
    "        next_state_values = self.calculate_next_q_values(next_states_v)\n",
    "        next_state_values = next_state_values.max(1)[0].detach()\n",
    "        expected_state_action_values = next_state_values * self.gamma + rewards_v\n",
    "        \n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "    def train_actor(self, train_epochs=10):\n",
    "        total_loss = 0.\n",
    "        # train Q network on new samples \n",
    "        optimizer = optim.Adam(self.q_network.parameters())\n",
    "        for epoch in range(train_epochs):\n",
    "            batch = self.memory.sample_batch()\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.q_network_loss(batch)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(self.q_network.parameters(), 1.0, norm_type=1)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return (total_loss / train_epochs)\n",
    "\n",
    "    def get_action_and_mutant(self, sample, epsilon):\n",
    "        sample_tensor = torch.FloatTensor([sample.ravel()])\n",
    "        prediction = self.calculate_next_q_values(sample_tensor).detach().numpy()\n",
    "        prediction = prediction.reshape((len(self.alphabet), self.seq_size))\n",
    "        # make action\n",
    "        moves = renormalize_moves(sample, prediction)\n",
    "        p = random.random()\n",
    "        action = sample_random(moves) if p < epsilon else sample_greedy(moves)\n",
    "        predicted_reward = (moves * action).sum()\n",
    "        # get next state (mutant)\n",
    "        mutant = construct_mutant_from_sample(action, sample)\n",
    "        mutant_string = translate_one_hot_to_string(mutant, self.alphabet)\n",
    "\n",
    "        return action, mutant, predicted_reward\n",
    "    \n",
    "    def rollout(self, sample, epsilon, steps=5):\n",
    "        results = []\n",
    "        mutant = sample.copy()\n",
    "        for _ in range(steps):\n",
    "            prev_mutant = mutant.copy()\n",
    "            action, mutant, predicted_reward = self.get_action_and_mutant(mutant, epsilon)\n",
    "            results.append((prev_mutant, action, mutant, predicted_reward))\n",
    "        return results\n",
    "    \n",
    "    def initialize_actor_network(self):\n",
    "        # initialization of actor network with randomly selected samples \n",
    "        samples = generate_random_sequences(length, self.experiment_batch_size, alphabet=RAA)\n",
    "        samples = np.array([translate_string_to_one_hot(state_string, self.alphabet) \n",
    "                                    for state_string in samples])\n",
    "        for i in range(self.experiment_batch_size):\n",
    "            state = samples[i]\n",
    "            action, new_state, _ = self.get_action_and_mutant(state, 0.5)\n",
    "            new_state_string = translate_one_hot_to_string(new_state, self.alphabet)\n",
    "            actual_reward = self.landscape.get_fitness(new_state_string)\n",
    "            self.memory.store(state.ravel(), action.ravel(), actual_reward, new_state.ravel())\n",
    "        avg_loss = self.train_actor(self.train_epochs)\n",
    "        print('Initialization of actor network finished...')\n",
    "        return samples\n",
    "        \n",
    "    def pick_action(self, samples):\n",
    "        eps = max(self.epsilon_min, (0.5 - self.landscape.cost / (self.experiment_batch_size * generations)))\n",
    "        b = 0\n",
    "        new_samples = []\n",
    "        costs, fitnesses = [], []\n",
    "        # perform rollout procedure to prepare next batch used in experimentation \n",
    "        for i in range(self.experiment_batch_size):\n",
    "            rollout_results = self.rollout(samples[i], eps)\n",
    "            new_samples.extend(rollout_results)\n",
    "        new_samples = sorted(new_samples, key = lambda x: x[3])[-self.experiment_batch_size:]\n",
    "#         print(len(set([translate_one_hot_to_string(sample[2], self.alphabet) for sample in new_samples])))\n",
    "        # evaluate next batch \n",
    "        for i in range(self.experiment_batch_size):\n",
    "            state, action, new_state, _ = new_samples[i]\n",
    "            new_state_string = translate_one_hot_to_string(new_state, self.alphabet)\n",
    "            actual_reward = self.landscape.get_fitness(new_state_string)\n",
    "            if not new_state_string in self.landscape.measured_sequences:\n",
    "                if actual_reward > self.best_fitness:\n",
    "                    state_tensor = torch.FloatTensor([state.ravel()])\n",
    "                    prediction = self.calculate_next_q_values(state_tensor).detach().numpy()\n",
    "                    prediction = prediction.reshape((len(self.alphabet), self.seq_size))\n",
    "                    #print(prediction)\n",
    "                    self.top_sequence.append((actual_reward, new_state, self.landscape.cost))\n",
    "                self.best_fitness = max(self.best_fitness, actual_reward)\n",
    "                self.memory.store(state.ravel(), action.ravel(), actual_reward, new_state.ravel())\n",
    "        # set new set of samples to be evaluated in next iterations of loop\n",
    "        samples = [sample[2] for sample in new_samples] \n",
    "        #print([translate_one_hot_to_string(sample, self.alphabet) for sample in samples])\n",
    "        avg_loss = self.train_actor(self.train_epochs)\n",
    "        costs.append(self.landscape.cost)\n",
    "        fitnesses.append(self.best_fitness)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_Network(\n",
      "  (linear1): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (bn1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear2): Linear(in_features=160, out_features=40, bias=True)\n",
      "  (bn2): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=40, out_features=1, bias=True)\n",
      ")\n",
      "Initialization of actor network finished...\n",
      "Cost: 30, Top fitness: 0.16117646834429572\n",
      "Cost: 39, Top fitness: 0.16823529636158663\n",
      "Cost: 46, Top fitness: 0.16823529636158663\n",
      "Cost: 50, Top fitness: 0.16823529636158663\n",
      "Cost: 54, Top fitness: 0.16823529636158663\n",
      "Cost: 64, Top fitness: 0.16823529636158663\n",
      "Cost: 74, Top fitness: 0.1929411719827091\n",
      "Cost: 83, Top fitness: 0.1929411719827091\n",
      "Cost: 90, Top fitness: 0.1929411719827091\n",
      "Cost: 97, Top fitness: 0.1929411719827091\n",
      "Cost: 104, Top fitness: 0.1929411719827091\n"
     ]
    }
   ],
   "source": [
    "agent = RL_agent_DQN(landscape=noisy_landscape, start_sequence=wt,\n",
    "                     alphabet=RAA, gamma=0.8, memory_size=10000,\n",
    "                     device=device, experiment_batch_size=experiment_batch_size)\n",
    "\n",
    "noisy_landscape.reset()\n",
    "noisy_landscape.measure_true_landscape(initial_genotypes)\n",
    "noisy_landscape.natural_mode=False\n",
    "noisy_landscape.local_mode=False\n",
    "noisy_landscape.cost\n",
    "\n",
    "samples = agent.initialize_actor_network()\n",
    "while noisy_landscape.cost < generations*experiment_batch_size:\n",
    "    samples = agent.pick_action(samples)\n",
    "    print(f\"Cost: {noisy_landscape.cost}, Top fitness: {agent.top_sequence[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "#plt.xscale(\"log\")\n",
    "plt.title(f\"Batch size: {experiment_batch_size}; Generations: {generations}; Noise: {noise_alpha}\")\n",
    "plt.xlabel(\"number of batches tested\")\n",
    "plt.ylabel(\"maximum fitness observed\")\n",
    "plt.plot([t[2]/experiment_batch_size for t in agent.top_sequence[1:]],[t[0] for t in agent.top_sequence[1:]],label=\"RL DQN PER better explorer\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xticks(range(generations));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_Network(\n",
      "  (linear1): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (bn1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear2): Linear(in_features=160, out_features=40, bias=True)\n",
      "  (bn2): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=40, out_features=1, bias=True)\n",
      ")\n",
      "Initialization of actor network finished...\n",
      "Cost: 22, Top fitness: 0.16000000448787913\n",
      "Cost: 24, Top fitness: 0.16000000448787913\n",
      "Cost: 25, Top fitness: 0.16000000448787913\n",
      "Cost: 29, Top fitness: 0.18235294117647058\n",
      "Cost: 31, Top fitness: 0.18235294117647058\n",
      "Cost: 32, Top fitness: 0.18235294117647058\n",
      "Cost: 33, Top fitness: 0.18235294117647058\n",
      "Cost: 33, Top fitness: 0.18235294117647058\n",
      "Cost: 35, Top fitness: 0.18235294117647058\n",
      "Cost: 35, Top fitness: 0.18235294117647058\n",
      "Cost: 36, Top fitness: 0.18235294117647058\n",
      "Cost: 38, Top fitness: 0.18235294117647058\n",
      "Cost: 38, Top fitness: 0.18235294117647058\n",
      "Cost: 38, Top fitness: 0.18235294117647058\n",
      "Cost: 39, Top fitness: 0.18235294117647058\n",
      "Cost: 39, Top fitness: 0.18235294117647058\n",
      "Cost: 40, Top fitness: 0.18235294117647058\n",
      "Cost: 40, Top fitness: 0.18235294117647058\n",
      "Cost: 41, Top fitness: 0.18235294117647058\n",
      "Cost: 42, Top fitness: 0.22941176470588234\n",
      "Cost: 42, Top fitness: 0.22941176470588234\n",
      "Cost: 42, Top fitness: 0.22941176470588234\n",
      "Cost: 43, Top fitness: 0.22941176470588234\n",
      "Cost: 43, Top fitness: 0.22941176470588234\n",
      "Cost: 44, Top fitness: 0.22941176470588234\n",
      "Cost: 46, Top fitness: 0.22941176470588234\n",
      "Cost: 46, Top fitness: 0.22941176470588234\n",
      "Cost: 46, Top fitness: 0.22941176470588234\n",
      "Cost: 47, Top fitness: 0.22941176470588234\n",
      "Cost: 48, Top fitness: 0.22941176470588234\n",
      "Cost: 48, Top fitness: 0.22941176470588234\n",
      "Cost: 49, Top fitness: 0.22941176470588234\n",
      "Cost: 49, Top fitness: 0.22941176470588234\n",
      "Cost: 49, Top fitness: 0.22941176470588234\n",
      "Cost: 50, Top fitness: 0.22941176470588234\n",
      "Cost: 50, Top fitness: 0.22941176470588234\n",
      "Cost: 50, Top fitness: 0.22941176470588234\n",
      "Cost: 50, Top fitness: 0.22941176470588234\n",
      "Cost: 51, Top fitness: 0.22941176470588234\n",
      "Cost: 52, Top fitness: 0.22941176470588234\n",
      "Cost: 53, Top fitness: 0.22941176470588234\n",
      "Cost: 54, Top fitness: 0.22941176470588234\n",
      "Cost: 55, Top fitness: 0.22941176470588234\n",
      "Cost: 55, Top fitness: 0.22941176470588234\n",
      "Cost: 56, Top fitness: 0.22941176470588234\n",
      "Cost: 56, Top fitness: 0.22941176470588234\n",
      "Cost: 57, Top fitness: 0.22941176470588234\n",
      "Cost: 57, Top fitness: 0.22941176470588234\n",
      "Cost: 59, Top fitness: 0.22941176470588234\n",
      "Cost: 60, Top fitness: 0.22941176470588234\n",
      "Cost: 61, Top fitness: 0.22941176470588234\n",
      "Cost: 63, Top fitness: 0.22941176470588234\n",
      "Cost: 63, Top fitness: 0.22941176470588234\n",
      "Cost: 64, Top fitness: 0.22941176470588234\n",
      "Cost: 65, Top fitness: 0.22941176470588234\n",
      "Cost: 65, Top fitness: 0.22941176470588234\n",
      "Cost: 65, Top fitness: 0.22941176470588234\n",
      "Cost: 65, Top fitness: 0.22941176470588234\n",
      "Cost: 66, Top fitness: 0.22941176470588234\n",
      "Cost: 67, Top fitness: 0.22941176470588234\n",
      "Cost: 67, Top fitness: 0.22941176470588234\n",
      "Cost: 67, Top fitness: 0.22941176470588234\n",
      "Cost: 67, Top fitness: 0.22941176470588234\n",
      "Cost: 67, Top fitness: 0.22941176470588234\n",
      "Cost: 69, Top fitness: 0.22941176470588234\n",
      "Cost: 70, Top fitness: 0.22941176470588234\n",
      "Cost: 70, Top fitness: 0.22941176470588234\n",
      "Cost: 70, Top fitness: 0.22941176470588234\n",
      "Cost: 70, Top fitness: 0.22941176470588234\n",
      "Cost: 71, Top fitness: 0.22941176470588234\n",
      "Cost: 72, Top fitness: 0.22941176470588234\n",
      "Cost: 72, Top fitness: 0.22941176470588234\n",
      "Cost: 75, Top fitness: 0.22941176470588234\n",
      "Cost: 76, Top fitness: 0.22941176470588234\n",
      "Cost: 77, Top fitness: 0.22941176470588234\n",
      "Cost: 77, Top fitness: 0.22941176470588234\n",
      "Cost: 77, Top fitness: 0.22941176470588234\n",
      "Cost: 77, Top fitness: 0.22941176470588234\n",
      "Cost: 77, Top fitness: 0.22941176470588234\n",
      "Cost: 78, Top fitness: 0.22941176470588234\n",
      "Cost: 80, Top fitness: 0.22941176470588234\n",
      "Cost: 81, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 82, Top fitness: 0.22941176470588234\n",
      "Cost: 83, Top fitness: 0.22941176470588234\n",
      "Cost: 85, Top fitness: 0.22941176470588234\n",
      "Cost: 85, Top fitness: 0.22941176470588234\n",
      "Cost: 87, Top fitness: 0.22941176470588234\n",
      "Cost: 88, Top fitness: 0.22941176470588234\n",
      "Cost: 89, Top fitness: 0.22941176470588234\n",
      "Cost: 90, Top fitness: 0.22941176470588234\n",
      "Cost: 90, Top fitness: 0.22941176470588234\n",
      "Cost: 92, Top fitness: 0.22941176470588234\n",
      "Cost: 92, Top fitness: 0.22941176470588234\n",
      "Cost: 92, Top fitness: 0.22941176470588234\n",
      "Cost: 93, Top fitness: 0.22941176470588234\n",
      "Cost: 94, Top fitness: 0.22941176470588234\n",
      "Cost: 94, Top fitness: 0.22941176470588234\n",
      "Cost: 94, Top fitness: 0.22941176470588234\n",
      "Cost: 95, Top fitness: 0.22941176470588234\n",
      "Cost: 96, Top fitness: 0.22941176470588234\n",
      "Cost: 96, Top fitness: 0.22941176470588234\n",
      "Cost: 96, Top fitness: 0.22941176470588234\n",
      "Cost: 97, Top fitness: 0.22941176470588234\n",
      "Cost: 97, Top fitness: 0.22941176470588234\n",
      "Cost: 98, Top fitness: 0.22941176470588234\n",
      "Cost: 98, Top fitness: 0.22941176470588234\n",
      "Cost: 98, Top fitness: 0.22941176470588234\n",
      "Cost: 99, Top fitness: 0.22941176470588234\n",
      "Cost: 99, Top fitness: 0.22941176470588234\n",
      "Cost: 101, Top fitness: 0.22941176470588234\n"
     ]
    }
   ],
   "source": [
    "agent = RL_agent_DQN(landscape=noisy_landscape, start_sequence=wt,\n",
    "                     alphabet=RAA, gamma=0, memory_size=10000,\n",
    "                     device=device, experiment_batch_size=experiment_batch_size)\n",
    "\n",
    "noisy_landscape.reset()\n",
    "noisy_landscape.measure_true_landscape(initial_genotypes)\n",
    "noisy_landscape.natural_mode=False\n",
    "noisy_landscape.local_mode=False\n",
    "noisy_landscape.cost\n",
    "\n",
    "samples = agent.initialize_actor_network()\n",
    "while noisy_landscape.cost < generations*experiment_batch_size:\n",
    "    samples = agent.pick_action(samples)\n",
    "    print(f\"Cost: {noisy_landscape.cost}, Top fitness: {agent.top_sequence[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
