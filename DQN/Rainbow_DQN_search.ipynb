{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os \n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import editdistance\n",
    "import sys\n",
    "import RNA\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# import path \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.sequence_utils import translate_one_hot_to_string,generate_random_mutant\n",
    "from models.Theoretical_models import *\n",
    "from models.Noise_wrapper import *\n",
    "from exploration_strategies.CE import *\n",
    "from utils.landscape_utils import *\n",
    "from models.RNA_landscapes import *\n",
    "from models.Multi_dimensional_model import *\n",
    "\n",
    "from segment_tree import MinSegmentTree, SumSegmentTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCCGCGGUACAAUGAUUUCGGAGUGUGGCGCGUAACGCCC\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "RAA=\"UGCA\" #alphabet\n",
    "length=40\n",
    "wt=generate_random_sequences(length,1,alphabet=RAA)[0]\n",
    "print(wt)\n",
    "#make a simple folding landscape starting at wt\n",
    "landscape1=RNA_landscape(wt)\n",
    "noise_alpha=1\n",
    "virtual_per_measure_ratio=15\n",
    "temperature=0.1\n",
    "# there are multiple abstract \"noise models\" you can use, or you can try to train your own model, using skM\n",
    "noisy_landscape_CE=Noise_wrapper(landscape1,noise_alpha=noise_alpha)\n",
    "noisy_landscape_RL=Noise_wrapper(landscape1,noise_alpha=noise_alpha)\n",
    "noisy_landscape_RL_multiple=Noise_wrapper(landscape1,noise_alpha=noise_alpha)\n",
    "#noisy_landscape=Gaussian_noise_landscape(base_landscape,noise_alpha=0.15)\n",
    "#noisy_landscape=DF_noise_landscape(base_landscape,noise_alpha=0.5)\n",
    "batch_size = 1000\n",
    "initial_genotypes=list(set([wt]+[generate_random_mutant(wt,0.05,RAA) for i in range(batch_size)]))[:batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow DQN RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import bisect\n",
    "from utils.sequence_utils import translate_string_to_one_hot, translate_one_hot_to_string\n",
    "\n",
    "def renormalize_moves(one_hot_input, rewards_output):\n",
    "    \"\"\"ensures that staying in place gives no reward\"\"\"\n",
    "    zero_current_state = (one_hot_input - 1) * (-1)\n",
    "    return np.multiply(rewards_output, zero_current_state)\n",
    "\n",
    "def walk_away_renormalize_moves(one_hot_input, one_hot_wt, rewards_output):\n",
    "    \"\"\"ensures that moving toward wt is also not useful\"\"\"\n",
    "    zero_current_state=(one_hot_input-1)*-1\n",
    "    zero_wt=((one_hot_wt-1)*-1)\n",
    "    zero_conservative_moves=np.multiply(zero_wt,zero_current_state)\n",
    "    return np.multiply(rewards_output,zero_conservative_moves)\n",
    "\n",
    "def get_all_singles_fitness(model,sequence,alphabet):\n",
    "    prob_singles=np.zeros((len(alphabet),len(sequence)))\n",
    "    for i in range(len(sequence)):\n",
    "        for j in range(len(alphabet)):\n",
    "            putative_seq=sequence[:i]+alphabet[j]+sequence[i+1:]\n",
    "           # print (putative_seq)\n",
    "            prob_singles[j][i]=model.get_fitness(putative_seq)\n",
    "    return prob_singles\n",
    "\n",
    "def get_all_mutants(sequence):\n",
    "    mutants = []\n",
    "    for i in range(sequence.shape[0]):\n",
    "        for j in range(sequence.shape[1]):\n",
    "            putative_seq = sequence.copy()\n",
    "            putative_seq[:, j] = 0\n",
    "            putative_seq[i, j] = 1\n",
    "            mutants.append(putative_seq)\n",
    "    return np.array(mutants)\n",
    "\n",
    "def sample_greedy(matrix):\n",
    "    i,j=matrix.shape\n",
    "    max_arg=np.argmax(matrix)\n",
    "    y=max_arg%j\n",
    "    x=int(max_arg/j)\n",
    "    output=np.zeros((i,j))\n",
    "    output[x][y]=matrix[x][y]\n",
    "    return output\n",
    "\n",
    "def sample_multi_greedy(matrix):\n",
    "    n = 5 # the number of base positions to greedily change\n",
    "    max_args = np.argpartition(matrix.flatten(), -n)[-n:]\n",
    "    i,j=matrix.shape\n",
    "    output=np.zeros((i,j))\n",
    "    for max_arg in max_args:\n",
    "        y=max_arg%j\n",
    "        x=int(max_arg/j)\n",
    "        output[x][y]=matrix[x][y]\n",
    "    return output\n",
    "\n",
    "def sample_random(matrix):\n",
    "    i,j=matrix.shape\n",
    "    non_zero_moves=np.nonzero(matrix)\n",
    "   # print (non_zero_moves)\n",
    "    k=len(non_zero_moves)\n",
    "    l=len(non_zero_moves[0])\n",
    "    if k!=0 and l!=0:\n",
    "        rand_arg=random.choice([[non_zero_moves[alph][pos] for alph in range(k)] for pos in range(l)])\n",
    "    else:\n",
    "        rand_arg=[random.randint(0,i-1),random.randint(0,j-1)]\n",
    "    #print (rand_arg)\n",
    "    y=rand_arg[1]\n",
    "    x=rand_arg[0]\n",
    "    output=np.zeros((i,j))\n",
    "    output[x][y] = 1\n",
    "    return output   \n",
    "\n",
    "def action_to_scalar(matrix):\n",
    "    matrix = matrix.ravel()\n",
    "    for i in range(len(matrix)):\n",
    "        if matrix[i] != 0:\n",
    "            return i\n",
    "    \n",
    "def construct_mutant_from_sample(pwm_sample, one_hot_base):\n",
    "    one_hot = np.zeros(one_hot_base.shape)\n",
    "    one_hot += one_hot_base\n",
    "    nonzero = np.nonzero(pwm_sample)\n",
    "    nonzero = list(zip(nonzero[0], nonzero[1]))\n",
    "    for nz in nonzero: # this can be problematic for non-positive fitnesses\n",
    "        i, j = nz\n",
    "        one_hot[:,j]=0\n",
    "        one_hot[i,j]=1\n",
    "    return one_hot\n",
    "\n",
    "def best_predicted_new_gen(actor, genotypes, alphabet, pop_size):\n",
    "    mutants = get_all_mutants(genotypes)\n",
    "    one_hot_mutants = np.array([translate_string_to_one_hot(mutant, alphabet) for mutant in mutants])\n",
    "    torch_one_hot_mutants = torch.from_numpy(np.expand_dims(one_hot_mutants, axis=0)).float()\n",
    "    predictions = actor(torch_one_hot_mutants)\n",
    "    predictions = predictions.detach().numpy()\n",
    "    best_pred_ind = predictions.argsort()[-pop_size:]\n",
    "    return mutants[best_pred_ind]\n",
    "\n",
    "def make_one_hot_train_test(genotypes, model, alphabet):\n",
    "    genotypes_one_hot = np.array([translate_string_to_one_hot(genotype, alphabet) for genotype in genotypes])\n",
    "    genotype_fitnesses = []\n",
    "    for genotype in genotypes:\n",
    "        genotype_fitnesses.append(model.get_fitness(genotype))\n",
    "    genotype_fitnesses = np.array(genotype_fitnesses)\n",
    "\n",
    "    return genotypes_one_hot, genotype_fitnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_tree import MinSegmentTree, SumSegmentTree\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim, \n",
    "        size, \n",
    "        batch_size = 128, \n",
    "        n_step = 1, \n",
    "        gamma = 0.99\n",
    "    ):\n",
    "        self.obs_buf = np.zeros((size,) + obs_dim, dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((size,) + obs_dim, dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "        \n",
    "        # for N-step Learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def store(\n",
    "        self, \n",
    "        obs, \n",
    "        act, \n",
    "        rew, \n",
    "        next_obs\n",
    "    ):\n",
    "        transition = (obs, act, rew, next_obs)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        # single step transition is not ready\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return False\n",
    "        \n",
    "        # make a n-step transition\n",
    "        rew, next_obs = self._get_n_step_info(\n",
    "            self.n_step_buffer, self.gamma\n",
    "        )\n",
    "        obs, action = self.n_step_buffer[0][:2]\n",
    "        \n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def sample_batch(self):\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size)\n",
    "\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            indices=idxs,\n",
    "        )\n",
    "    \n",
    "    def sample_batch_from_idxs(self, idxs):\n",
    "        # for N-step Learning\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs]\n",
    "        )\n",
    "    \n",
    "    def _get_n_step_info(\n",
    "        self, n_step_buffer, gamma):\n",
    "        \"\"\"Return n step rew, next_obs.\"\"\"\n",
    "        # info of the last transition\n",
    "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
    "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
    "            r, next_obs = transition[-2:]\n",
    "            rew = r + gamma * rew\n",
    "\n",
    "        return rew, next_obs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    \"\"\"Prioritized Replay buffer.\n",
    "    \n",
    "    Attributes:\n",
    "        max_priority (float): max priority\n",
    "        tree_ptr (int): next index of tree\n",
    "        alpha (float): alpha parameter for prioritized replay buffer\n",
    "        sum_tree (SumSegmentTree): sum tree for prior\n",
    "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim,\n",
    "        size, \n",
    "        batch_size = 128, \n",
    "        alpha = 0.6\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        assert alpha >= 0\n",
    "        \n",
    "        super(PrioritizedReplayBuffer, self).__init__(obs_dim, size, batch_size)\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "        \n",
    "    def store(self, obs, act, rew, next_obs):\n",
    "        \"\"\"Store experience and priority.\"\"\"\n",
    "        transition = super().store(obs, act, rew, next_obs)\n",
    "        \n",
    "        self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "        self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "        self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "        \n",
    "        return transition\n",
    "\n",
    "    def sample_batch(self, beta = 0.4):\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        indices = self._sample_proportional()\n",
    "        \n",
    "        obs = self.obs_buf[indices]\n",
    "        next_obs = self.next_obs_buf[indices]\n",
    "        acts = self.acts_buf[indices]\n",
    "        rews = self.rews_buf[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        \n",
    "        return dict(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            acts=acts,\n",
    "            rews=rews,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "        \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "            \n",
    "    def _sample_proportional(self):\n",
    "        \"\"\"Sample indices based on proportions.\"\"\"\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment = p_total / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def _calculate_weight(self, idx, beta):\n",
    "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "        \n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear module for NoisyNet.\n",
    "    \n",
    "    Attributes:\n",
    "        in_features (int): input size of linear module\n",
    "        out_features (int): output size of linear module\n",
    "        std_init (float): initial std value\n",
    "        weight_mu (nn.Parameter): mean value weight parameter\n",
    "        weight_sigma (nn.Parameter): std value weight parameter\n",
    "        bias_mu (nn.Parameter): mean value bias parameter\n",
    "        bias_sigma (nn.Parameter): std value bias parameter\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
    "        mu_range = 1 / np.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(\n",
    "            self.std_init / np.sqrt(self.in_features)\n",
    "        )\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(\n",
    "            self.std_init / np.sqrt(self.out_features)\n",
    "        )\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Make new noise.\"\"\"\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "\n",
    "        # outer product\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\n",
    "        \n",
    "        We don't use separate statements on train / eval mode.\n",
    "        It doesn't show remarkable difference of performance.\n",
    "        \"\"\"\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_noise(size: int) -> torch.Tensor:\n",
    "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
    "        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n",
    "\n",
    "        return x.sign().mul(x.abs().sqrt())\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        sequence_len, \n",
    "        alphabet_len,\n",
    "        atom_size, \n",
    "        support\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.support = support\n",
    "        self.dim = sequence_len*alphabet_len\n",
    "        self.atom_size = atom_size\n",
    "\n",
    "        # set common feature layer\n",
    "        num_moves = sequence_len*alphabet_len\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.dim), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # set advantage layer\n",
    "        self.advantage_hidden_layer = NoisyLinear(self.dim, self.dim)\n",
    "        self.advantage_layer = NoisyLinear(self.dim, self.dim*atom_size)\n",
    "\n",
    "        # set value layer\n",
    "        self.value_hidden_layer = NoisyLinear(self.dim, self.dim)\n",
    "        self.value_layer = NoisyLinear(self.dim, atom_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = x.view(-1, self.dim)\n",
    "        dist = self.dist(x)\n",
    "        q = torch.sum(dist * self.support, dim=2)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def dist(self, x):\n",
    "        \"\"\"Get distribution for atoms.\"\"\"\n",
    "        x = x.view(-1, self.dim)\n",
    "        feature = self.feature_layer(x)\n",
    "        adv_hid = F.relu(self.advantage_hidden_layer(feature))\n",
    "        val_hid = F.relu(self.value_hidden_layer(feature))\n",
    "        \n",
    "        advantage = self.advantage_layer(adv_hid).view(\n",
    "            -1, self.dim, self.atom_size\n",
    "        )\n",
    "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "        \n",
    "        return dist\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset all noisy layers.\"\"\"\n",
    "        self.advantage_hidden_layer.reset_noise()\n",
    "        self.advantage_layer.reset_noise()\n",
    "        self.value_hidden_layer.reset_noise()\n",
    "        self.value_layer.reset_noise()\n",
    "    \n",
    "def build_network(sequence_len, alphabet_len, atom_size, support, device):\n",
    "    model = Network(sequence_len, alphabet_len, atom_size, support).to(device)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "class RL_agent_Rainbow_DQN():\n",
    "    '''\n",
    "    Based off https://github.com/Curt-Park/rainbow-is-all-you-need/blob/master/08.rainbow.ipynb\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 start_sequence, \n",
    "                 alphabet, \n",
    "                 alpha = 0.6,\n",
    "                 beta = 0.4,\n",
    "                 gamma = 0.9, \n",
    "                 prior_eps = 1e-6,\n",
    "                 memory_size = 100000, \n",
    "                 batch_size = 128, \n",
    "                 v_min = 0.0,\n",
    "                 v_max = 10.0,\n",
    "                 atom_size = 51,\n",
    "                 n_step = 3,\n",
    "                 device = \"cpu\", \n",
    "                 noise_alpha=1):\n",
    "        self.alphabet = alphabet\n",
    "        self.state = translate_string_to_one_hot(start_sequence, self.alphabet)\n",
    "        self.seq_size = len(start_sequence)\n",
    "        self.device = device\n",
    "        # neural networks and their parameters \n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.atom_size = atom_size\n",
    "        self.support = torch.linspace(\n",
    "            self.v_min, self.v_max, self.atom_size\n",
    "        ).to(device)\n",
    "        self.net = build_network(self.seq_size, len(self.alphabet), atom_size, \n",
    "                                 self.support, device)\n",
    "        self.target_net = build_network(self.seq_size, len(self.alphabet), atom_size, \n",
    "                                        self.support, device)\n",
    "        self.target_net.load_state_dict(self.net.state_dict())\n",
    "        self.net.eval()\n",
    "        self.target_net.eval()\n",
    "        # other params\n",
    "        self.start_sequence = translate_string_to_one_hot(start_sequence,self.alphabet)\n",
    "        self.memory_size = memory_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        # 1-Step Learning\n",
    "        obs_dim = (len(self.alphabet), self.seq_size)\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            obs_dim, memory_size, self.batch_size, alpha=alpha\n",
    "        )\n",
    "        # N-Step Learning \n",
    "        self.n_step = n_step\n",
    "        self.use_n_step = True if n_step > 1 else False\n",
    "        if self.use_n_step:\n",
    "            self.n_step = n_step\n",
    "            self.memory_n = ReplayBuffer(\n",
    "                obs_dim, memory_size, self.batch_size, n_step=n_step, gamma=gamma\n",
    "            )\n",
    "\n",
    "        self.seen_sequences = []\n",
    "        self.landscape = Noise_wrapper(RNA_landscape(start_sequence), noise_alpha=noise_alpha)\n",
    "        self.best_fitness = 0\n",
    "\n",
    "    def reset_position(self,sequence):\n",
    "        self.state=translate_string_to_one_hot(sequence,self.alphabet)\n",
    "\n",
    "    def get_position(self):\n",
    "        return translate_one_hot_to_string(self.state,self.alphabet)\n",
    "\n",
    "    def translate_pwm_to_sequence(self,input_seq_one_hot,output_pwm):\n",
    "        diff=output_pwm-input_seq_one_hot\n",
    "        most_likely=np.argmax(diff,axis=0)\n",
    "        out_seq=\"\"\n",
    "        for m in most_likely:\n",
    "            out_seq+=self.alphabet[m]\n",
    "        return out_seq\n",
    "    \n",
    "    def q_network_loss(self, samples, gamma):\n",
    "        \"\"\"\n",
    "        Calculate MSE between actual state action values,\n",
    "        and expected state action values from DQN\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "\n",
    "        # Categorical DQN algorithm\n",
    "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_action = self.net(next_state).argmax(1)\n",
    "            next_dist = self.target_net.dist(next_state)\n",
    "            next_dist = next_dist[range(self.batch_size), next_action]\n",
    "\n",
    "            t_z = reward + gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "            offset = torch.linspace(\n",
    "                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n",
    "                ).long().unsqueeze(1).expand(self.batch_size, self.atom_size)\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.net.dist(state)\n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "\n",
    "        return elementwise_loss\n",
    "    \n",
    "    def train_actor(self, train_epochs=10):\n",
    "        total_loss = 0.\n",
    "        # train Q network on new samples \n",
    "        optimizer = optim.Adam(self.net.parameters())\n",
    "        for epoch in range(train_epochs):\n",
    "            # sample for 1-step learning loss first \n",
    "            samples = self.memory.sample_batch(self.beta)\n",
    "            weights = torch.FloatTensor(\n",
    "                samples[\"weights\"].reshape(-1, 1)\n",
    "            ).to(self.device)\n",
    "            indices = samples[\"indices\"]\n",
    "            elementwise_loss = self.q_network_loss(samples, self.gamma)\n",
    "            loss = torch.mean(elementwise_loss * weights)\n",
    "            # add in N-step learning loss if it is being used \n",
    "            if self.use_n_step:\n",
    "                gamma = self.gamma ** self.n_step\n",
    "                samples = self.memory_n.sample_batch_from_idxs(indices)\n",
    "                elementwise_loss_n_loss = self.q_network_loss(samples, gamma)\n",
    "                elementwise_loss += elementwise_loss_n_loss\n",
    "                loss = torch.mean(elementwise_loss * weights)\n",
    "            # train model \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # PER: update priorities\n",
    "            loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
    "            new_priorities = loss_for_prior + self.prior_eps\n",
    "            self.memory.update_priorities(indices, new_priorities)\n",
    "            total_loss += loss.item()\n",
    "            self.net.reset_noise()\n",
    "            self.target_net.reset_noise()\n",
    "        return (total_loss / train_epochs)\n",
    "\n",
    "    def pick_action(self):\n",
    "        # get action from current state, use noisy network instead of epsilon-greedy for exploration\n",
    "        action = self.net(torch.FloatTensor(self.state)).argmax()\n",
    "        action_matrix = np.zeros(len(self.alphabet) * self.seq_size)\n",
    "        action_matrix[action] = 1\n",
    "        action_matrix = action_matrix.reshape((len(self.alphabet), self.seq_size))\n",
    "        # get new state after action is performed \n",
    "        mutant = construct_mutant_from_sample(action_matrix, self.state)\n",
    "        mutant_string = translate_one_hot_to_string(mutant, self.alphabet)\n",
    "        self.state = mutant\n",
    "\n",
    "        return action, mutant\n",
    "    \n",
    "    def run_RL(self, generations=10, train_epochs=10):\n",
    "        while self.landscape.cost < self.batch_size*generations:\n",
    "            eps = max(0.05, (0.5 - self.landscape.cost / (self.batch_size * generations)))\n",
    "            b = 0\n",
    "            new = []\n",
    "            while(b < self.batch_size):\n",
    "                state = self.state.copy() \n",
    "                action, new_state = agent.pick_action()\n",
    "                new_state_string = translate_one_hot_to_string(new_state, self.alphabet)\n",
    "                reward = self.landscape.get_fitness(new_state_string)\n",
    "                if not new_state_string in self.landscape.measured_sequences:\n",
    "                    if reward > self.best_fitness:\n",
    "                        print(self.net(torch.tensor(self.state).float()).detach().numpy()[0])\n",
    "                        print(self.target_net(torch.tensor(self.state).float()).detach().numpy()[0])\n",
    "                    self.best_fitness = max(self.best_fitness, reward)\n",
    "                    # add N-step transition\n",
    "                    if self.use_n_step:\n",
    "                        is_n_step_stored = self.memory_n.store(state, action, reward, new_state) \n",
    "                    # add a single step transition\n",
    "                    if not self.use_n_step or is_n_step_stored:\n",
    "                        self.memory.store(state, action, reward, new_state)\n",
    "                    b += 1\n",
    "            # reset target network \n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "            \n",
    "            avg_loss = agent.train_actor(train_epochs)\n",
    "            print (self.landscape.cost, self.best_fitness, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "generations = 20\n",
    "agent = RL_agent_Rainbow_DQN(wt, alphabet=RAA, gamma=0.9, atom_size=10,\n",
    "                             batch_size=128, memory_size=10000, device=device)\n",
    "agent.run_RL(train_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
