{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%autoreload 2\n",
    "import bisect\n",
    "import copy \n",
    "import os \n",
    "from collections import deque, Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import editdistance\n",
    "import sys\n",
    "import RNA\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# import path \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.sequence_utils import translate_one_hot_to_string,generate_random_mutant\n",
    "from utils.sequence_utils import translate_string_to_one_hot, translate_one_hot_to_string\n",
    "from models.Theoretical_models import *\n",
    "from models.Noise_wrapper import *\n",
    "from exploration_strategies.CE import *\n",
    "from utils.landscape_utils import *\n",
    "from models.RNA_landscapes import *\n",
    "from models.Multi_dimensional_model import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.agents.ppo import ppo_policy, ppo_agent, ppo_utils\n",
    "from tf_agents.environments import py_environment, tf_py_environment\n",
    "from tf_agents.environments.utils import validate_py_environment\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.networks import network, normal_projection_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import actor_distribution_rnn_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import value_rnn_network\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAA=\"UGCA\" #alphabet\n",
    "alphabet_len=len(RAA)\n",
    "# TODO: UNDO THIS\n",
    "# length=40\n",
    "length=20\n",
    "noise_alpha=1\n",
    "generations = 10\n",
    "experiment_batch_size = 1000\n",
    "wt=generate_random_sequences(length,1,alphabet=RAA)[0]\n",
    "landscape1=RNA_landscape(wt)\n",
    "landscape2=RNA_landscape(wt)\n",
    "noisy_landscape=Noise_wrapper(landscape1,\n",
    "                              noise_alpha=noise_alpha,\n",
    "                              always_costly=True)\n",
    "initial_genotypes=list(set([wt]+[generate_random_mutant(wt,0.05,RAA) \n",
    "                                 for i in range(experiment_batch_size*10)]))[:experiment_batch_size]\n",
    "noisy_landscape.reset()\n",
    "noisy_landscape.measure_true_landscape(initial_genotypes)\n",
    "noisy_landscape.natural_mode=False\n",
    "noisy_landscape.local_mode=False\n",
    "noisy_landscape.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitnessLandscapeEnvironment(py_environment.PyEnvironment):\n",
    "    # Based on this: https://www.mikulskibartosz.name/how-to-create-an-environment-for-a-tensorflow-agent/\n",
    "    def __init__(self, alphabet, seq_len, landscape, max_episodes):\n",
    "        self.alphabet = alphabet\n",
    "        self.alphabet_len = len(self.alphabet)\n",
    "        self.landscape = copy.deepcopy(landscape)\n",
    "        self.seq_len = seq_len\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1, 2), dtype=np.float32, minimum=0, \n",
    "            maximum=1, name='action_x')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(self.alphabet_len, self.seq_len), dtype=np.float32, minimum=0, \n",
    "            maximum=1, name='observation')\n",
    "        self._time_step_spec = ts.time_step_spec(self._observation_spec)\n",
    "        self._state = translate_string_to_one_hot(wt, self.alphabet)\n",
    "        self._episode_ended = False\n",
    "        self.ctr = 0\n",
    "        self.max_episodes = max_episodes\n",
    "        self.seen_sequences = {}\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.ctr = 0\n",
    "        # TODO: UNDO THIS\n",
    "#         self._state = translate_string_to_one_hot(wt, self.alphabet)\n",
    "        self._state = translate_string_to_one_hot(generate_random_sequences(length,1,alphabet=RAA)[0], self.alphabet)\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
    "    \n",
    "    def time_step_spec(self):\n",
    "        return self._time_step_spec \n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def get_state_string(self):\n",
    "        return translate_one_hot_to_string(self._state, self.alphabet)\n",
    "    \n",
    "    def _step(self, action):\n",
    "        if self.ctr < self.max_episodes:\n",
    "            self.ctr += 1\n",
    "            action_one_hot = np.zeros((self.alphabet_len, self.seq_len))\n",
    "            if np.amax(action) > 1 or np.amin(action) < 0:\n",
    "                \n",
    "                self.seen_sequences = {}\n",
    "                return ts.termination(np.array(self._state, dtype=np.float32), 0)\n",
    "            x, y = action[0]\n",
    "            x, y = int(self.alphabet_len*x), int(self.seq_len*y)\n",
    "            action_one_hot[x, y] = 1\n",
    "            assert self._state.sum() == self._state.shape[1]\n",
    "            if self._state[x, y] == 1:\n",
    "                self._episode_ended = True\n",
    "                \n",
    "                self.seen_sequences = {}\n",
    "                return ts.termination(np.array(self._state, dtype=np.float32), 0)\n",
    "            else:\n",
    "#                 self._state = construct_mutant_from_sample(action_one_hot, self._state)\n",
    "                new_stage = construct_mutant_from_sample(action_one_hot, self._state)\n",
    "                state_string = translate_one_hot_to_string(new_stage, self.alphabet)\n",
    "                try_iter = 0 \n",
    "                while state_string in self.seen_sequences:\n",
    "                    try_iter += 1\n",
    "                    new_stage = construct_mutant_from_sample(action_one_hot, self._state)\n",
    "                    state_string = translate_one_hot_to_string(new_stage, self.alphabet)\n",
    "#                     print(\"www\")\n",
    "                    \n",
    "                    if try_iter >= 1000:\n",
    "                        self._state = translate_string_to_one_hot(wt, self.alphabet)\n",
    "                        self.seen_sequences = {}\n",
    "                        return ts.termination(np.array(self._state, dtype=np.float32), 0)\n",
    "                                        \n",
    "#                 if state_string in self.seen_sequences:\n",
    "#                     self._state = translate_string_to_one_hot(wt, self.alphabet)\n",
    "#                     self.seen_sequences = {}\n",
    "#                     return ts.termination(np.array(self._state, dtype=np.float32), 0)\n",
    "\n",
    "                assert state_string not in self.seen_sequences\n",
    "                self._state=new_stage\n",
    "                self.seen_sequences[state_string] = 1\n",
    "                \n",
    "                reward = self.landscape.get_fitness(state_string)\n",
    "                assert self._state.sum() == self._state.shape[1]\n",
    "                return ts.transition(np.array(self._state, dtype=np.float32), reward=reward)\n",
    "        else:\n",
    "            self._episode_ended = True\n",
    "            assert self._state.sum() == self._state.shape[1]\n",
    "            \n",
    "            self.seen_sequences = {}\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting validating environment...\n",
      "done validating environment.\n"
     ]
    }
   ],
   "source": [
    "#max_iter = experiment_batch_size * generations \n",
    "max_iter = 10 ** 6\n",
    "fle2 = FitnessLandscapeEnvironment(RAA, length, landscape1, max_iter)\n",
    "print(\"starting validating environment...\")\n",
    "validate_py_environment(fle2, episodes=2)\n",
    "print(\"done validating environment.\")\n",
    "fle = FitnessLandscapeEnvironment(RAA, length, landscape2, max_iter)\n",
    "tf_env = tf_py_environment.TFPyEnvironment(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-6-28837e2f79b0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-28837e2f79b0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# return\n",
    "%autoreload 2\n",
    "from PPO_utils import PPO_RL_Agent \n",
    "\n",
    "def BoostedEnvironment():\n",
    "    return FitnessLandscapeEnvironment(RAA, length, landscape2, max_iter)\n",
    "\n",
    "actor_fc_layers = (200, 100)\n",
    "value_fc_layers = (200, 100)\n",
    "env_load_fn = BoostedEnvironment\n",
    "'''\n",
    "actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    input_fc_layer_params=actor_fc_layers,\n",
    "    output_fc_layer_params=None\n",
    ")\n",
    "value_net = value_rnn_network.ValueRnnNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    input_fc_layer_params=value_fc_layers,\n",
    "    output_fc_layer_params=None\n",
    ")\n",
    "'''\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "  tf_env.observation_spec(),\n",
    "  tf_env.action_spec(),\n",
    "  fc_layer_params=actor_fc_layers)\n",
    "value_net = value_network.ValueNetwork(\n",
    "  tf_env.observation_spec(), fc_layer_params=value_fc_layers)\n",
    "num = 100000\n",
    "ppo_rl_agent = PPO_RL_Agent(env_load_fn, actor_net, value_net, RAA, num_environment_steps=500000)\n",
    "ppo_rl_agent.run_train()\n",
    "batch = ppo_rl_agent.pick_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_dist = get_distance_fitness(wt, noisy_landscape)\n",
    "plt.scatter([x[0] for x in PPO_dist], [x[1] for x in PPO_dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python RNA_3.6",
   "language": "python",
   "name": "rna_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
